{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from scipy.io import arff\n",
    "from cStringIO import StringIO\n",
    "import pandas\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "import sklearn.metrics.cluster as sk_cluster_m\n",
    "import sklearn.metrics as skmetrics\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Figure, Layout\n",
    "import math\n",
    "from sklearn.metrics import adjusted_rand_score, calinski_harabaz_score\n",
    "import matplotlib.patches as mpatches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function Utils\n",
    "def rand_centroids(K, X):\n",
    "    # rand_centroids(K=Int, X=Float_array):\n",
    "    # Return a numpy array of size K with each element \n",
    "    # being a normally random distributed var with mu and sigma calculated \n",
    "    # from the mean and std of the data X\n",
    "    mean, std = np.mean(X, axis=0), np.std(X, axis=0)\n",
    "    centroids = [np.random.normal(mean, std) for n in range(K)]\n",
    "    return np.array(centroids)\n",
    "\n",
    "def euc_distance(X, Y):    \n",
    "    # euc_distance(X=Float_array, Y=Float_array):\n",
    "    # Returns an array of euclidean distances, \n",
    "    # for the square root of the sum of the square of the differences\n",
    "    # of array X and array Y\n",
    "    diff = X - Y[:, np.newaxis]\n",
    "    squared_diff = diff**2\n",
    "    sum_squared_diff = squared_diff.sum(axis=2)\n",
    "    return np.sqrt(sum_squared_diff)\n",
    "\n",
    "def compute_centroids(K, C, X):\n",
    "    # compute_centroids(K=Int, C=Float_array, X=Float_array)\n",
    "    # Compute the centroids for cluster size K, centroid(s) C and data X\n",
    "    # where a new centroid is calculated as the mean of the data points \n",
    "    # which share a common nearest centroid. Repeats until the sum of\n",
    "    # the euc distances between centroids and points does not change, \n",
    "    # then returns the cluster of centroids\n",
    "    D = euc_distance(X, C)\n",
    "    CC = np.argmin(D, axis=0)\n",
    "    C = np.array([new_centroid(k, X, CC) for k in range(K)])\n",
    "    D2 = euc_distance(X, C)\n",
    "    if (D.sum() == D2.sum()):\n",
    "        return C, np.argmin(D2, axis=0)\n",
    "    else:\n",
    "        return compute_centroids(K, C, X)\n",
    "\n",
    "def new_centroid(k, X, CC):\n",
    "    # Returns a new centroid based on the mean of the points associated with it\n",
    "    # if no points associated with it, generates a new one\n",
    "    x = X[CC==k]\n",
    "    if (len(x) > 0):\n",
    "        return x.mean(axis=0)\n",
    "    else: \n",
    "        return rand_centroids(1, X)[0]\n",
    "    \n",
    "def k_means(K, X, display_title=True):\n",
    "    # k_means(K=Int, X=Float_array)\n",
    "    # K-means for clust size K on dataset X using random initialised centroids\n",
    "    # returns final cluster and predicted labels\n",
    "    if display_title:\n",
    "        print \"Computing K-means with K=\",K\n",
    "    C = rand_centroids(K, X)\n",
    "    return compute_centroids(K, C, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bk_means(X, K, k_means_iter=1, log_level=1):\n",
    "    #Bk_means(X=ndarray, K=Int, k_means_iter=3)\n",
    "    #X: data to cluster\n",
    "    #K: number of clusters\n",
    "    #k_means_iter: number of iteretations on the kmeans call. # of split cluster pairs.\n",
    "    #log_level: 0 : no messages, 1 print messages \n",
    "    \n",
    "    # Initialize cluster  assigment with all data\n",
    "    clusters=np.zeros((X.shape[0],1))\n",
    "    \n",
    "    print \"Computing Bisecting Kmeans with K=\",K,\"Iterations of internal kmeans splitting:\",k_means_iter\n",
    "    #Set initial number of cluster to 1 and iterate until number of clusters=K\n",
    "    \n",
    "    for k in range(1,K):\n",
    "        if log_level:\n",
    "            print \"*********** NEW ITERATION ************* \", k\n",
    "        similarity=[]\n",
    "        potential_new_clusters={}\n",
    "\n",
    "        if log_level:\n",
    "            print \"*********select cluster to split******\"\n",
    "        \n",
    "        larger_cluster_index = select_split_cluster(clusters,\"larger\",log_level) #options: larger, heterogeny, \n",
    "        if log_level:\n",
    "            print \"Selected cluster: \", larger_cluster_index\n",
    "        \n",
    "\n",
    "        kmeans_data=X[np.where(clusters==larger_cluster_index),:]\n",
    "        kmeans_data=kmeans_data[0]\n",
    "        \n",
    "        if log_level:\n",
    "            print \"*********Generate 2 clusters with Kmeans ******\"\n",
    "            print \"*********Best of \", k_means_iter,\" results ******\"\n",
    "        \n",
    "        for i in range(0,k_means_iter): \n",
    "        #if k_means_iter >1 then we select best k_means split with similarity \n",
    "            #potential_new_clusters[i] = KMeans(2, \"random\",1).fit_predict(kmeans_data)\n",
    "            #potential_new_clusters[i] = KMeans(2).fit_predict(kmeans_data)\n",
    "            #call to our kmeans function\n",
    "            if log_level: \n",
    "                print k_means(2,kmeans_data)\n",
    "            potential_new_clusters[i] = k_means(2,kmeans_data,display_title=False)[1]\n",
    "            similarity.append(cost_function(kmeans_data,potential_new_clusters[i],log_level))\n",
    "        \n",
    "        #Select division based on similarity (min value max similarity)\n",
    "        selected_division=potential_new_clusters[similarity.index(min(similarity))]\n",
    "        \n",
    "        if log_level:\n",
    "            print \"Selected case: \", similarity.index(min(similarity))\n",
    "        \n",
    "        new_clusters=selected_division\n",
    "        new_clusters[np.where(selected_division==1)]=k\n",
    "        new_clusters[np.where(selected_division==0)]=larger_cluster_index\n",
    "\n",
    "        \n",
    "        clusters[np.where(clusters[:]==larger_cluster_index)]=new_clusters\n",
    "\n",
    "    if log_level:\n",
    "        print \"****** END OF BKmeans *********\\n\\n\\n\"    \n",
    "    return clusters.flatten()\n",
    "\n",
    "def squares_dist(x):\n",
    "    #squares_dist(x=ndarray):\n",
    "    #x: matrix N,M. N rows of data variables. \n",
    "    #.  M/2 columns are data features values, M/2 columns are cluster centroid coord. \n",
    "    #returns square distances\n",
    "    \n",
    "    return (sp.spatial.distance.pdist([x[:x.shape[0]/2],x[x.shape[0]/2:]], 'euclidean'))**2\n",
    "\n",
    "\n",
    "def cost_function(data_n,clusters_n, log_level=1):\n",
    "    #cost_function(data_n=ndarray,clusters_n:array)\n",
    "    #data_n: clustered data\n",
    "    #clusters_n: clusters association in the data (size: rows of data_n, 1)\n",
    "    #returns the cost function value\n",
    "    \n",
    "    #Find the centroids of each cluster \n",
    "    mus=np.array([data_n[np.where(clusters_n==k)].mean(axis=0) for k in range(len(np.unique(clusters_n)))])\n",
    "    \n",
    "    #vector of mu feature values of the associated cluster for each data variable\n",
    "    mus_complete=np.empty([clusters_n.shape[0],data_n.shape[1]])\n",
    "    \n",
    "    for k in range(len(np.unique(clusters_n))):\n",
    "        \n",
    "        mus_complete[np.where(clusters_n==k)]=mus[k]\n",
    "    \n",
    "    #calculate cost function\n",
    "    cost_f=sum(np.apply_along_axis(squares_dist, axis=1,arr=np.concatenate((data_n,mus_complete),axis=1))) \n",
    "    cost_f=cost_f/data_n.shape[0]\n",
    "    \n",
    "    if log_level:\n",
    "        print \"Cost function kmeans split:\",cost_f\n",
    "    return cost_f\n",
    "\n",
    "def select_split_cluster(clusters, criteria=\"larger\", log_level=1):\n",
    "    #select_split_cluster (clusters, criteria)\n",
    "    #clusters: vector of data cluster association (1 column)\n",
    "    #criteria= \"larger\". (more option tbi)\n",
    "    #returns de number of the selected cluster.\n",
    "    \n",
    "    selected_key_c=0\n",
    "    number_of_x=[]\n",
    "    \n",
    "    if criteria == \"larger\":\n",
    "        for i in np.nditer(np.unique(clusters)):\n",
    "\n",
    "            number_of_x.append([len(clusters[np.where(clusters==i)]),i])\n",
    "        \n",
    "        selected_key_c=number_of_x[number_of_x.index(max(number_of_x))][1]\n",
    "        \n",
    "        if log_level:\n",
    "            print \"Number of x in each cluster:\", number_of_x\n",
    "\n",
    "    return selected_key_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fcm(data, cluster_n, expo=2, max_iter=100, min_impro=1e-5, display=True,\n",
    "        initU=None):\n",
    "    # Data set clustering using fuzzy c-means clustering.\n",
    "    # The Function finds N_CLUSTER number of clusters in the data set DATA.\n",
    "    # DATA is size M-by-N, where M is the number of data points and N is the\n",
    "    # number of coordinates for each data point. The coordinates for each\n",
    "    # cluster center are returned in the rows of the matrix CENTER.\n",
    "    # The membership function matrix U contains the grade of membership of\n",
    "    # each DATA point in each cluster. The values 0 and 1 indicate no\n",
    "    # membership and full membership respectively. Grades between 0 and 1\n",
    "    # indicate that the data point has partial membership in a cluster.\n",
    "    # At each iteration, an objective function is minimized to find the best\n",
    "    # location for the clusters and its values are returned in OBJ_FCN.\n",
    "\n",
    "    print \"Computing FCM with K=\",cluster_n,\"and m=\",expo\n",
    "    \n",
    "    data_n, in_n = data.shape\n",
    "    obj_fcn = np.zeros((max_iter, 1))\n",
    "\n",
    "    # Perform checks\n",
    "    assert expo != 1, \"Exponential parameter must be different from 1\"\n",
    "\n",
    "    U = initfcm(cluster_n, data_n, initU)\n",
    "    for i in range(max_iter):\n",
    "        U, center, obj_fcn[i] = stepfcm(data, U, cluster_n, expo)\n",
    "        if display:\n",
    "            print(\"Iteration count {}, obj. fcn = {}\".format(i, obj_fcn[i][0]))\n",
    "        # check termination condition\n",
    "        if i > 0:\n",
    "            if np.abs(obj_fcn[i] - obj_fcn[i-1]) < min_impro:\n",
    "                break\n",
    "\n",
    "    obj_fcn = obj_fcn[:(i + 1)]  # Actual number of iterations\n",
    "    labels = np.argmax(U, axis=0)\n",
    "    return center, U, obj_fcn, labels\n",
    "\n",
    "\n",
    "def stepfcm(data, U, cluster_n, expo):\n",
    "    # One step in fuzzy c-mean clustering.\n",
    "    # performs one iteration of fuzzy c-mean clustering, where\n",
    "    #\n",
    "    # DATA: matrix of data to be clustered. (Each row is a data point.)\n",
    "    # U: partition matrix. (U(i,j) is the MF value of data j in cluster i.)\n",
    "    # CLUSTER_N: number of clusters.\n",
    "    # EXPO: exponent (> 1) for the partition matrix.\n",
    "    # U_NEW: new partition matrix.\n",
    "    # CENTER: center of clusters. (Each row is a center.)\n",
    "\n",
    "    # MF matrix after exponential modification\n",
    "    mf = np.power(U, expo)\n",
    "\n",
    "    # New center\n",
    "    center = (np.dot(mf, data)\n",
    "              / mf.sum(axis=1, keepdims=True) * np.ones((1, data.shape[1])))\n",
    "\n",
    "    # fill the distance matrix\n",
    "    dist = distfcm(center, data)\n",
    "\n",
    "    # objective function\n",
    "    obj_fcn = np.sum(np.power(dist, 2) * mf)\n",
    "\n",
    "    # calculate new U, suppose expo != 1\n",
    "    tmp = np.power(dist, -2/(expo-1))\n",
    "    U_new = tmp/(np.ones((cluster_n, 1))*np.sum(tmp, axis=0, keepdims=True))\n",
    "\n",
    "    return U_new, center, obj_fcn\n",
    "\n",
    "\n",
    "def distfcm(center, data):\n",
    "    # Distance measure in fuzzy c-mean clustering.\n",
    "    # Calculates the Euclidean distance between each row in CENTER and each\n",
    "    # row in DATA, and returns a distance matrix OUT of size M by N, where\n",
    "    # M and N are row dimensions of CENTER and DATA, respectively, and\n",
    "    # OUT(I, J) is the distance between CENTER(I,:) and DATA(J,:).\n",
    "    out = np.zeros((center.shape[0], data.shape[0]))\n",
    "\n",
    "    # fill the output matrix\n",
    "    if center.shape[1] > 1:\n",
    "        for k in range(center.shape[0]):\n",
    "            out[k, :] = np.sqrt(np.sum(np.power(\n",
    "                data - np.ones((data.shape[0], 1)) * center[k, :]\n",
    "                , 2), axis=1))\n",
    "    else:  # 1-D data\n",
    "        for k in range(center.shape[0]):\n",
    "            out[k, :] = np.abs(center[k]-data).transpose()\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def initfcm(cluster_n, data_n, initU=None):\n",
    "    # Generate initial fuzzy partition matrix for fuzzy c-means clustering.\n",
    "    # If initU is provided, the function uses initU as the U matrix, otherwise\n",
    "    # the function randomly generates a fuzzy partition matrix U that is\n",
    "    # CLUSTER_N by DATA_N, where CLUSTER_N is number of clusters and\n",
    "    # DATA_N is number of data points. The summation of each column of the\n",
    "    # generated U is equal to unity, as required by fuzzy c-means clustering.\n",
    "\n",
    "    if initU is None:\n",
    "        U = np.random.rand(cluster_n, data_n)\n",
    "    else:\n",
    "        assert isinstance(initU, np.ndarray), \"initU must be a numpy.ndarray\"\n",
    "        assert (cluster_n, data_n) == initU.shape, \\\n",
    "            \"The dimensions of initU must be ({},{})\".format(cluster_n, data_n)\n",
    "        U = initU\n",
    "\n",
    "    U = U/U.sum(axis=0, keepdims=True)\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing K-means with K= 2\n",
      "Computing Bisecting Kmeans with K= 2 Iterations of internal kmeans splitting: 1\n",
      "Computing K-means with K= 3\n",
      "Computing Bisecting Kmeans with K= 3 Iterations of internal kmeans splitting: 1\n",
      "Computing K-means with K= 4\n",
      "Computing Bisecting Kmeans with K= 4 Iterations of internal kmeans splitting: 1\n",
      "Computing K-means with K= 5\n",
      "Computing Bisecting Kmeans with K= 5 Iterations of internal kmeans splitting: 1\n",
      "Computing K-means with K= 6\n",
      "Computing Bisecting Kmeans with K= 6 Iterations of internal kmeans splitting: 1\n",
      "Computing K-means with K= 7\n",
      "Computing Bisecting Kmeans with K= 7 Iterations of internal kmeans splitting: 1\n",
      "Computing K-means with K= 8\n",
      "Computing Bisecting Kmeans with K= 8 Iterations of internal kmeans splitting: 1\n",
      "Computing K-means with K= 9\n",
      "Computing Bisecting Kmeans with K= 9 Iterations of internal kmeans splitting: 1\n",
      "Computing K-means with K= 10\n",
      "Computing Bisecting Kmeans with K= 10 Iterations of internal kmeans splitting: 1\n",
      "Computing K-means with K= 11\n",
      "Computing Bisecting Kmeans with K= 11 Iterations of internal kmeans splitting: 1\n",
      "Computing K-means with K= 12\n",
      "Computing Bisecting Kmeans with K= 12 Iterations of internal kmeans splitting: 1\n",
      "Computing K-means with K= 13\n",
      "Computing Bisecting Kmeans with K= 13 Iterations of internal kmeans splitting: 1\n"
     ]
    }
   ],
   "source": [
    "# Data Import\n",
    "iris, iris_meta = arff.loadarff(\"datasets/iris.arff\")\n",
    "iris_data = np.array([iris['sepallength'], iris['sepalwidth'],\n",
    "                      iris['petallength'], iris['petalwidth']]).transpose()\n",
    "iris_class = iris['class'].reshape((150, 1))\n",
    "\n",
    "\n",
    "penbased, penbased_meta = arff.loadarff(\"datasets/pen-based.arff\")\n",
    "penbased_data=np.zeros((len(penbased), len(penbased[0])-1))\n",
    "penbased_class=np.zeros((len(penbased)))\n",
    "\n",
    "i=0\n",
    "for d in penbased:\n",
    "    penbased_data[i,:]=[d[0],d[1],d[2],d[3],d[4],d[5],d[6],d[7],d[8],d[9],d[10],d[11],d[12],d[13],d[14],d[15]]\n",
    "    penbased_class[i]=penbased[16][0]                \n",
    "    i=i+1  \n",
    "\n",
    "\n",
    "def compare_cluster_methods(data, labels_true, k_range=range(2, 21)):\n",
    "\n",
    "    ch_scores_km = []; ch_scores_bkm = []; ch_scores_fcm = []\n",
    "    ar_scores_km = []; ar_scores_bkm = []; ar_scores_fcm = []\n",
    "    _, lab_codes = np.unique(labels_true, return_inverse=True)\n",
    "\n",
    "    for k in k_range:\n",
    "        _, labels_km = k_means(k, data)\n",
    "        labels_bkm = Bk_means(data, k, log_level=0)\n",
    "        _, _, _, labels_fcm = fcm(data, k,display=False)\n",
    "\n",
    "        ch_scores_km.append(calinski_harabaz_score(data, labels_km))\n",
    "        ch_scores_bkm.append(calinski_harabaz_score(data, labels_bkm))\n",
    "        ch_scores_fcm.append(calinski_harabaz_score(data, labels_fcm))\n",
    "\n",
    "        ar_scores_km.append(adjusted_rand_score(lab_codes, labels_km))\n",
    "        ar_scores_bkm.append(adjusted_rand_score(lab_codes, labels_bkm))\n",
    "        ar_scores_fcm.append(adjusted_rand_score(lab_codes, labels_fcm))\n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "    f.suptitle('Cluster validation metrics')\n",
    "    ax1.plot(k_range, ch_scores_km, 'r',\n",
    "             k_range, ch_scores_bkm, 'g',\n",
    "             k_range, ch_scores_fcm, 'b')\n",
    "    ax1.set_ylabel('Calinski-Harabaz Index')\n",
    "    ax1.grid(linestyle='--', linewidth=0.5)\n",
    "    ax1.legend(handles=[\n",
    "        mpatches.Patch(color='r', label='K-Means'),\n",
    "        mpatches.Patch(color='g', label='Bis. K-Means'),\n",
    "        mpatches.Patch(color='b', label='Fuzzy C-Means')\n",
    "    ], loc='upper right')\n",
    "\n",
    "    ax2.plot(k_range, ar_scores_km, 'r',\n",
    "             k_range, ar_scores_bkm, 'g',\n",
    "             k_range, ar_scores_fcm, 'b')\n",
    "    ax2.set_ylabel('Adjusted Rand Score')\n",
    "    ax2.set_xlabel('Number of clusters')\n",
    "    ax2.set_xlim(np.min(k_range), np.max(k_range))\n",
    "    ax2.grid(linestyle='--', linewidth=0.5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#compare_cluster_methods(iris_data, iris_class)\n",
    "compare_cluster_methods(penbased_data, penbased_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
