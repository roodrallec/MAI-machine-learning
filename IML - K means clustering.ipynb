{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.arff import loadarff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function Utils\n",
    "def rand_centroids(K, X):\n",
    "    # rand_centroids(K=Int, X=Float_array):\n",
    "    # Return a numpy array of size K with each element \n",
    "    # being a normally random distributed var with mu and sigma calculated \n",
    "    # from the mean and std of the data X\n",
    "    mean, std = np.mean(X, axis=0), np.std(X, axis=0)\n",
    "    clusters = [np.random.normal(mean, std) for n in range(K)]\n",
    "    return np.array(clusters)\n",
    "\n",
    "def euc_distance(X, Y):    \n",
    "    # euc_distance(X=Float_array, Y=Float_array):\n",
    "    # Returns an array of euclidean distances, \n",
    "    # for the square root of the sum of the square of the differences\n",
    "    # of array X and array Y\n",
    "    diff = X - Y[:, np.newaxis]\n",
    "    squared_diff = diff**2\n",
    "    sum_squared_diff = squared_diff.sum(axis=2)\n",
    "    return np.sqrt(sum_squared_diff)\n",
    "\n",
    "def compute_clusters(K, C, X):\n",
    "    # compute_clusters(K=Int, C=Float_array, X=Float_array)\n",
    "    # Compute the clusters for cluster size K, clusters C and data X\n",
    "    # where a new cluster is calculated as the mean of the data points \n",
    "    # which share a common nearest cluster. Repeats\n",
    "    # until the sum of the euc distances between clusters\n",
    "    # and points does not change\n",
    "    print('=> OldCluster:', C)\n",
    "    D = euc_distance(X, C)\n",
    "    CC = np.argmin(D, axis=0)\n",
    "    C = np.array([X[CC==k].mean(axis=0) for k in range(K)])\n",
    "    print('=> NewCluster:', C)\n",
    "    D2 = euc_distance(X, C)\n",
    "    print('=> DistanceSums:', D.sum(), D2.sum())\n",
    "    if (D.sum() == D2.sum()):\n",
    "        return C\n",
    "    else:\n",
    "        return compute_clusters(K, C, X)\n",
    "\n",
    "def k_means(K, X):\n",
    "    # k_means(K=Int, X=Float_array)\n",
    "    # K-means for clust size K on dataset X using random initialised centroids\n",
    "    C = rand_centroids(K, X)\n",
    "    return compute_clusters(K, C, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris data set values into a numpy array\n",
    "iris = loadarff('./datasets/iris.arff')\n",
    "headers = iris[1]\n",
    "values = iris[0]\n",
    "iris_data = []\n",
    "for val in values:\n",
    "    iris_data.append([val[0], val[1], val[2], val[3]]) \n",
    "iris_data = np.array(iris_data).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('=> OldCluster:', array([[ 6.04833039,  3.29899686,  0.63365464,  0.65314573],\n",
      "       [ 5.8665646 ,  2.68828682,  2.44194179,  1.01416518]]))\n",
      "('=> NewCluster:', array([[ 5.02272727,  3.45454545,  1.43636364,  0.23636364],\n",
      "       [ 6.18396226,  2.88773585,  4.72264151,  1.59811321]]))\n",
      "('=> DistanceSums:', 880.11748145615582, 724.1276926276804)\n",
      "('=> OldCluster:', array([[ 5.02272727,  3.45454545,  1.43636364,  0.23636364],\n",
      "       [ 6.18396226,  2.88773585,  4.72264151,  1.59811321]]))\n",
      "('=> NewCluster:', array([[ 5.00784314,  3.4       ,  1.49411765,  0.26078431],\n",
      "       [ 6.27373737,  2.87575758,  4.92525253,  1.68181818]]))\n",
      "('=> DistanceSums:', 724.1276926276804, 728.73145837830452)\n",
      "('=> OldCluster:', array([[ 5.00784314,  3.4       ,  1.49411765,  0.26078431],\n",
      "       [ 6.27373737,  2.87575758,  4.92525253,  1.68181818]]))\n",
      "('=> NewCluster:', array([[ 5.00566038,  3.36037736,  1.56226415,  0.28867925],\n",
      "       [ 6.30103093,  2.88659794,  4.95876289,  1.69587629]]))\n",
      "('=> DistanceSums:', 728.73145837830452, 724.440100532408)\n",
      "('=> OldCluster:', array([[ 5.00566038,  3.36037736,  1.56226415,  0.28867925],\n",
      "       [ 6.30103093,  2.88659794,  4.95876289,  1.69587629]]))\n",
      "('=> NewCluster:', array([[ 5.00566038,  3.36037736,  1.56226415,  0.28867925],\n",
      "       [ 6.30103093,  2.88659794,  4.95876289,  1.69587629]]))\n",
      "('=> DistanceSums:', 724.440100532408, 724.440100532408)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5.00566038,  3.36037736,  1.56226415,  0.28867925],\n",
       "       [ 6.30103093,  2.88659794,  4.95876289,  1.69587629]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform k-means\n",
    "k_means(2, iris_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
