{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.arff import loadarff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function Utils\n",
    "def rand_centroids(K, X):\n",
    "    # rand_centroids(K=Int, X=Float_array):\n",
    "    # Return a numpy array of size K with each element \n",
    "    # being a normally random distributed var with mu and sigma calculated \n",
    "    # from the mean and std of the data X\n",
    "    mean, std = np.mean(X, axis=0), np.std(X, axis=0)\n",
    "    centroids = [np.random.normal(mean, std) for n in range(K)]\n",
    "    return np.array(centroids)\n",
    "\n",
    "def euc_distance(X, Y):    \n",
    "    # euc_distance(X=Float_array, Y=Float_array):\n",
    "    # Returns an array of euclidean distances, \n",
    "    # for the square root of the sum of the square of the differences\n",
    "    # of array X and array Y\n",
    "    diff = X - Y[:, np.newaxis]\n",
    "    squared_diff = diff**2\n",
    "    sum_squared_diff = squared_diff.sum(axis=2)\n",
    "    return np.sqrt(sum_squared_diff)\n",
    "\n",
    "def compute_centroids(K, C, X):\n",
    "    # compute_centroids(K=Int, C=Float_array, X=Float_array)\n",
    "    # Compute the centroids for cluster size K, centroid(s) C and data X\n",
    "    # where a new centroid is calculated as the mean of the data points \n",
    "    # which share a common nearest centroid. Repeats until the sum of\n",
    "    # the euc distances between centroids and points does not change, \n",
    "    # then returns the cluster of centroids\n",
    "    D = euc_distance(X, C)\n",
    "    CC = np.argmin(D, axis=0)\n",
    "    C = np.array([new_centroid(k, X, CC) for k in range(K)])\n",
    "    D2 = euc_distance(X, C)\n",
    "    if (D.sum() == D2.sum()):\n",
    "        return C, np.argmin(D2, axis=0)\n",
    "    else:\n",
    "        return compute_centroids(K, C, X)\n",
    "\n",
    "def new_centroid(k, X, CC):\n",
    "    # Returns a new centroid based on the mean of the points associated with it\n",
    "    # if no points associated with it, generates a new one\n",
    "    x = X[CC==k]\n",
    "    if (len(x) > 0):\n",
    "        return x.mean(axis=0)\n",
    "    else: \n",
    "        return rand_centroids(1, X)[0]\n",
    "    \n",
    "def k_means(K, X):\n",
    "    # k_means(K=Int, X=Float_array)\n",
    "    # K-means for clust size K on dataset X using random initialised centroids\n",
    "    # returns final cluster and predicted labels\n",
    "    C = rand_centroids(K, X)\n",
    "    return compute_centroids(K, C, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris data set values into a numpy array\n",
    "iris_data, iris_meta = loadarff('./datasets/iris.arff')\n",
    "data = np.array([[v[0], v[1], v[2], v[3]] for v in iris_data])\n",
    "labels = np.unique([v[4] for v in iris_data])\n",
    "labels_true = np.array([np.where(labels == v[4])[0][0] for v in iris_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.30103093  2.88659794  4.95876289  1.69587629]\n",
      " [ 5.00566038  3.36037736  1.56226415  0.28867925]]\n"
     ]
    }
   ],
   "source": [
    "# Perform k-means test\n",
    "K = 2\n",
    "clusters, labels_pred = k_means(K, data)\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.00566038  3.36037736  1.56226415  0.28867925]\n",
      " [ 6.30103093  2.88659794  4.95876289  1.69587629]]\n"
     ]
    }
   ],
   "source": [
    "# SK-learn k-means comparison test\n",
    "from sklearn.cluster import KMeans\n",
    "sk_means = KMeans(n_clusters=K, init='random').fit(data)\n",
    "print(sk_means.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53992182942071232"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjusted_rand_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "adjusted_rand_score(labels_true, labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513.30384335175665"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calinski harabaz score\n",
    "from sklearn.metrics import calinski_harabaz_score\n",
    "calinski_harabaz_score(data, labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform scores for each metric and several clusters\n",
    "MAX_K = 20\n",
    "accuracies = []\n",
    "for k in range(2, MAX_K):\n",
    "    clusters, labels_pred = k_means(k, data)\n",
    "    rand_score = adjusted_rand_score(labels_true, labels_pred)\n",
    "    calinski = calinski_harabaz_score(data, labels_pred)\n",
    "    accuracies.append([rand_score, calinski])\n",
    "accuracies = np.array(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///Users/jnalexander/Projects/MAI-machine-learning/temp-plot.html'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot the accuracies on a graph \n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Figure, Layout\n",
    "\n",
    "traces = [\n",
    "    Scatter(x=range(2, MAX_K), y=accuracies[:,0], name = 'adjusted_rand_score'),\n",
    "    Scatter(x=range(2, MAX_K), y=accuracies[:,1], name = 'calinski_score', yaxis='y2')\n",
    "]\n",
    "\n",
    "yaxis2=dict(side='right')\n",
    "layout = Layout(\n",
    "    title='K-means Metrics over cluster size 2-20',\n",
    "    xaxis=dict(title='cluster_size'),\n",
    "    yaxis=dict(title='adjusted_rand_score'),\n",
    "    yaxis2=dict(title='calinski_score', overlaying='y', side='right')\n",
    ")\n",
    "\n",
    "fig = Figure(data=traces, layout=layout)\n",
    "plot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nDetails about the implementation of your algorithms, including the decisions made during the\\nimplementation and the setup of the different parameters\\n\\nK-means\\n\\nThe implementation was mainly built from following the course notes on k-means, \\nas well as Coursera's tutorial:\\nhttps://www.coursera.org/learn/machine-learning/lecture/93VPG/k-means-algorithm\\n\\nThe algorithm was divided into 5 key functions:\\n    k_means(K, X)\\n        - for initializing a new clustering, K is the number of clusters wanted, X is the dataset\\n        to cluster.\\n        - the function simply initiates the centroids randomly and then computes the clusters.\\n        - the function returns both the clusters and an array of the predicted cluster assignment\\n        for each datapoint.\\n\\n    rand_centroids(K, X): \\n        - this function returns K generated centroids from data X.\\n        - it does this by calculating the standard deviation and mean along each dimension of X\\n        and then creates a centroid by choosing a normally distributed random value with the \\n        standard deviation and mean of the dimension for the centroid.\\n        - a normal distribution was chosen as this would provide a better initial guess, where\\n        better means that the centroid would be closer and converge faster to the final clusters.\\n    \\n    euc_distance(X, Y):   \\n        - this function returns the square root of the sum of the squares of the differences in \\n        values of X and Y, across the same dimensions. It was used as it is a distance metric for \\n        computing the distances from points and clusters in the k-means algorithm.\\n\\n    compute_centroids(K, C, X):\\n        - given the cluster size, initial centroids and data, this function computes the centroids\\n        and the array of the predicted centroid assignment for each datapoint.\\n        - it uses a euclidean distance metric to calculate the distance between centroids and\\n        points, then it assigns each point to its nearest centroid, before calling itself again,\\n        until a cluster array is produced that has the same euclidean distance sum as the one\\n        before it. \\n    \\n    new_centroid(k, X, CC):\\n        - this function creates a new centroid according to the cluster index, the data points\\n        and the map of closest centroids to points. \\n        - it does this by selecting the points associated with a centroid, calculating their mean,\\n        along each dimension, and returning this as an array.\\n        - important to note is that if a centroid is chosen that has no points associated with it,\\n        it was chosen to create a new centroid, based on the mean and var of the data \\n        (see rand_centroids function). This was done to ensure that the k-means algorithm always\\n        returned the same cluster size as was called for.\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Details about the implementation of your algorithms, including the decisions made during the\n",
    "implementation and the setup of the different parameters\n",
    "\n",
    "K-means\n",
    "\n",
    "The implementation was mainly built from following the course notes on k-means, \n",
    "as well as Coursera's tutorial:\n",
    "https://www.coursera.org/learn/machine-learning/lecture/93VPG/k-means-algorithm\n",
    "\n",
    "The algorithm was divided into 5 key functions:\n",
    "    k_means(K, X)\n",
    "        - for initializing a new clustering, K is the number of clusters wanted, X is the dataset\n",
    "        to cluster.\n",
    "        - the function simply initiates the centroids randomly and then computes the clusters.\n",
    "        - the function returns both the clusters and an array of the predicted cluster assignment\n",
    "        for each datapoint.\n",
    "\n",
    "    rand_centroids(K, X): \n",
    "        - this function returns K generated centroids from data X.\n",
    "        - it does this by calculating the standard deviation and mean along each dimension of X\n",
    "        and then creates a centroid by choosing a normally distributed random value with the \n",
    "        standard deviation and mean of the dimension for the centroid.\n",
    "        - a normal distribution was chosen as this would provide a better initial guess, where\n",
    "        better means that the centroid would be closer and converge faster to the final clusters.\n",
    "    \n",
    "    euc_distance(X, Y):   \n",
    "        - this function returns the square root of the sum of the squares of the differences in \n",
    "        values of X and Y, across the same dimensions. It was used as it is a distance metric for \n",
    "        computing the distances from points and clusters in the k-means algorithm.\n",
    "\n",
    "    compute_centroids(K, C, X):\n",
    "        - given the cluster size, initial centroids and data, this function computes the centroids\n",
    "        and the array of the predicted centroid assignment for each datapoint.\n",
    "        - it uses a euclidean distance metric to calculate the distance between centroids and\n",
    "        points, then it assigns each point to its nearest centroid, before calling itself again,\n",
    "        until a cluster array is produced that has the same euclidean distance sum as the one\n",
    "        before it. \n",
    "    \n",
    "    new_centroid(k, X, CC):\n",
    "        - this function creates a new centroid according to the cluster index, the data points\n",
    "        and the map of closest centroids to points. \n",
    "        - it does this by selecting the points associated with a centroid, calculating their mean,\n",
    "        along each dimension, and returning this as an array.\n",
    "        - important to note is that if a centroid is chosen that has no points associated with it,\n",
    "        it was chosen to create a new centroid, based on the mean and var of the data \n",
    "        (see rand_centroids function). This was done to ensure that the k-means algorithm always\n",
    "        returned the same cluster size as was called for.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
